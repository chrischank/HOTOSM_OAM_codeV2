{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "994b046f-e5e7-48c9-9afd-a510f864b5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "#Training loop for NNs       #\n",
    "#Maintainer: Christopher Chan#\n",
    "#Version: 0.1.0              #\n",
    "#Date: 2022-02-23            #\n",
    "##############################\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pathlib\n",
    "import time\n",
    "import re\n",
    "import PIL\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from Networks import Five_UNet\n",
    "from dataloader import BuildingDataset\n",
    "\n",
    "device = (torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "          else torch.device(\"cpu\"))\n",
    "\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "td_KBY = os.path.abspath(\"/home/chris/Dropbox/HOTOSM/SAMPLE/td_KBY\")\n",
    "td_DZK = os.path.abspath(\"/home/chris/Dropbox/HOTOSM/SAMPLE/td_DZK\")\n",
    "td_DZKN = os.path.abspath(\"/home/chris/Dropbox/HOTOSM/SAMPLE/td_DZKN\")\n",
    "\n",
    "#td_KBY = os.path.abspath(\"/home/mnt/HOTOSM_data/Kakuma/Kalobeyei/td_KBY\")\n",
    "#td_DZK = os.path.abspath(\"/home/mnt/HOTOSM_data/Dzaleka/td_DZK\")\n",
    "#td_DZKN = os.path.abspath(\"/home/mnt/HOTOSM_data/Dzaleka_N/td_DZKN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b4df71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512)\n",
      "(512, 512)\n",
      "(512, 512)\n",
      "(512, 512)\n",
      "(512, 512)\n",
      "(512, 512)\n",
      "(512, 512)\n",
      "(512, 512)\n",
      "(512, 512)\n",
      "(512, 512)\n"
     ]
    }
   ],
   "source": [
    "# Remove non square (512 x 512 imgs)\n",
    "\n",
    "for root, dirname, filename in os.walk(os.path.join(td_KBY, \"LBL\")):\n",
    "    for i in filename:\n",
    "        if i.endswith(\"png\"):\n",
    "            with PIL.Image.open(root + \"/\" + i) as img:\n",
    "                print(img.size)\n",
    "                \n",
    "                if img.size != (512, 512):\n",
    "                    os.remove(root + \"/\" + i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba42bb2-e3e5-4b82-8757-2c591377fa0c",
   "metadata": {},
   "source": [
    "### Train Val Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2cfbd46-14a4-46d0-955c-8bcb85c3d3f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "<class 'list'> <class 'list'>\n",
      "Total images and labels pair in DataLoader: 29\n",
      "Concatenated TRAINING images and labels pair: 17 :\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZK/IMG/DZK_IMG_1705-6138.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZK/LBL/DZK_LBL_1705-6138.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZK/IMG/DZK_IMG_1705-5797.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZK/LBL/DZK_LBL_1705-5797.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZK/IMG/DZK_IMG_1705-5456.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZK/LBL/DZK_LBL_1705-5456.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZK/IMG/DZK_IMG_1364-5797.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZK/LBL/DZK_LBL_1364-5797.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZK/IMG/DZK_IMG_1705-4774.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZK/LBL/DZK_LBL_1705-4774.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZK/IMG/DZK_IMG_1364-5456.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZK/LBL/DZK_LBL_1364-5456.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_KBY/IMG/KBY_IMG_2046-27962.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_KBY/LBL/KBY_LBL_2046-27962.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_KBY/IMG/KBY_IMG_2387-27962.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_KBY/LBL/KBY_LBL_2387-27962.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_KBY/IMG/KBY_IMG_2046-28303.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_KBY/LBL/KBY_LBL_2046-28303.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_KBY/IMG/KBY_IMG_2387-26939.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_KBY/LBL/KBY_LBL_2387-26939.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_KBY/IMG/KBY_IMG_2387-27621.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_KBY/LBL/KBY_LBL_2387-27621.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_KBY/IMG/KBY_IMG_2387-25234.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_KBY/LBL/KBY_LBL_2387-25234.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZKN/IMG/DZKN_IMG_0-9889.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZKN/LBL/DZKN_LBL_0-9889.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZKN/IMG/DZKN_IMG_0-9548.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZKN/LBL/DZKN_LBL_0-9548.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZKN/IMG/DZKN_IMG_341-9548.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZKN/LBL/DZKN_LBL_341-9548.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZKN/IMG/DZKN_IMG_682-10230.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZKN/LBL/DZKN_LBL_682-10230.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZKN/IMG/DZKN_IMG_1023-9889.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZKN/LBL/DZKN_LBL_1023-9889.png\n",
      "Concatenated VALIDATION images and labels pair: 9 :\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZK/IMG/DZK_IMG_1364-6138.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZK/LBL/DZK_LBL_1364-6138.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZK/IMG/DZK_IMG_1705-5115.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZK/LBL/DZK_LBL_1705-5115.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZK/IMG/DZK_IMG_1364-11594.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZK/LBL/DZK_LBL_1364-11594.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_KBY/IMG/KBY_IMG_2046-27621.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_KBY/LBL/KBY_LBL_2046-27621.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_KBY/IMG/KBY_IMG_2387-25575.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_KBY/LBL/KBY_LBL_2387-25575.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_KBY/IMG/KBY_IMG_2728-25234.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_KBY/LBL/KBY_LBL_2728-25234.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZKN/IMG/DZKN_IMG_341-9889.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZKN/LBL/DZKN_LBL_341-9889.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZKN/IMG/DZKN_IMG_1023-10230.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZKN/LBL/DZKN_LBL_1023-10230.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZKN/IMG/DZKN_IMG_682-9548.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZKN/LBL/DZKN_LBL_682-9548.png\n",
      "Concatenated TESTING images: 3 and labels pair: 3 :\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZK/IMG/DZK_IMG_1705-4433.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZK/LBL/DZK_LBL_1705-4433.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_KBY/IMG/KBY_IMG_2387-28303.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_KBY/LBL/KBY_LBL_2387-28303.png\n",
      "Image: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZKN/IMG/DZKN_IMG_682-9889.png Label: /home/chris/Dropbox/HOTOSM/SAMPLE/td_DZKN/LBL/DZKN_LBL_682-9889.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Below is a set of relatively complex functions which:\n",
    "# Perform the train, val, test split at a rounded ratio of 62%, 27%, and 10% based on each sets of imagery\n",
    "# This will be followed by first pseudo changing the name of _LBL_ to _IMG_ to match the split imagery\n",
    "# Lastly, once the correct LBL files are matched, \n",
    "\n",
    "def tvt_split(td):\n",
    "    \n",
    "    img_ls = []\n",
    "    \n",
    "    for root, dirs, filename in os.walk(os.path.join(td, \"IMG\")):\n",
    "        for i in filename:\n",
    "            if i.endswith(\".png\"):\n",
    "                img_ls.append(root + \"/\" + i)\n",
    "        \n",
    "        img_ls = BuildingDataset(img_ls, _)\n",
    "        \n",
    "        train_IMG, val_IMG, test_IMG = random_split(img_ls.png_dir, [int(round(0.6 * len(img_ls.png_dir))),\n",
    "                                                                     int(round(0.3 * len(img_ls.png_dir))),\n",
    "                                                                     int(round(0.1 * len(img_ls.png_dir)))])\n",
    "        \n",
    "        return train_IMG, val_IMG, test_IMG\n",
    "\n",
    "DZK_train, DZK_val, DZK_test = tvt_split(td_DZK)\n",
    "KBY_train, KBY_val, KBY_test = tvt_split(td_KBY)\n",
    "DZKN_train, DZKN_val, DZKN_test = tvt_split(td_DZKN)\n",
    "\n",
    "##################\n",
    "# TOO COMPLICATED#\n",
    "##################\n",
    "\n",
    "#def match_LBL(td, imgs):\n",
    "#    \n",
    "#    lbl_ls = []\n",
    "#    img_ls = []\n",
    "#    match_ls = []\n",
    "#    \n",
    "#    imgs = list(imgs)\n",
    "#    \n",
    "#    for root, dirs, filename in os.walk(os.path.join(td, \"LBL\")):\n",
    "#        for j in filename:\n",
    "#            if j.endswith(\".png\"):\n",
    "#                ps_name = j.rsplit(\"_LBL_\")[0] + \"_IMG_\" + j.rsplit(\"_LBL_\")[1] # Parse the string, PSEUDO-CHANGE _LBL_ to _IMG_\n",
    "#                lbl_ls.append(ps_name)\n",
    "#    \n",
    "#    for k in imgs:\n",
    "#        names = os.path.basename(k)\n",
    "#        img_ls.append(names)\n",
    "#        \n",
    "#    def common(a, b):\n",
    "#        a_set = set(a)\n",
    "#        b_set = set(b)\n",
    "#        if (a_set & b_set):\n",
    "#            return (a_set & b_set)\n",
    "#        else:\n",
    "#            print(\"No common elements\")\n",
    "#            \n",
    "#            \n",
    "#    match_ls = common(img_ls, lbl_ls)\n",
    "#        \n",
    "#    match_ls = [(root + \"/\" + n.replace(\"_IMG_\", \"_LBL_\")) for n in match_ls] # Change the _IMG_ back to _LBL_\n",
    "#    \n",
    "#    print(\"For the selected dataset of {0}, There are: {1} images, {2} labels, and {3} matching image/label pairs.\".format(os.path.basename(td), len(img_ls), len(lbl_ls), len(match_ls)))\n",
    "#    \n",
    "#    return match_ls\n",
    "\n",
    "#########################################\n",
    "# Assign matched LBL to new LBL datasets#\n",
    "#########################################\n",
    "\n",
    "#DZKLBL_Train = match_LBL(td_DZK, DZK_train)\n",
    "#DZKLBL_Val = match_LBL(td_DZK, DZK_val)\n",
    "#DZKLBL_Test = match_LBL(td_DZK, DZK_test)\n",
    "#DZKNLBL_Train = match_LBL(td_DZKN, DZKN_train)\n",
    "#DZKNLBL_Val = match_LBL(td_DZKN, DZKN_val)\n",
    "#DZKNLBL_Test = match_LBL(td_DZKN, DZKN_test)\n",
    "#KBYLBL_Train = match_LBL(td_KBY, KBY_train)\n",
    "#KBYLBL_Val = match_LBL(td_KBY, KBY_val)\n",
    "#KBYLBL_Test = match_LBL(td_KBY, KBY_test)\n",
    "\n",
    "############\n",
    "# Try again#\n",
    "############\n",
    "\n",
    "TrainLBL_ls = []\n",
    "ValLBL_ls = []\n",
    "TestLBL_ls = []\n",
    "\n",
    "TrainIMG_ls = list(DZK_train + KBY_train + DZKN_train)\n",
    "ValIMG_ls = list(DZK_val + KBY_val + DZKN_val)\n",
    "TestIMG_ls = list(DZK_test + KBY_test + DZKN_test)\n",
    "\n",
    "for i in TrainIMG_ls:\n",
    "    i = re.sub(\"IMG\", \"LBL\", i, count = 2)\n",
    "    TrainLBL_ls.append(i)\n",
    "\n",
    "for i in ValIMG_ls:\n",
    "    i = re.sub(\"IMG\", \"LBL\", i, count = 2)\n",
    "    ValLBL_ls.append(i)\n",
    "\n",
    "for i in TestIMG_ls:\n",
    "    i = re.sub(\"IMG\", \"LBL\", i, count = 2)\n",
    "    TestLBL_ls.append(i)\n",
    "\n",
    "Train = BuildingDataset(png_dir = TrainIMG_ls,\n",
    "                        lbl_dir = TrainLBL_ls)\n",
    "\n",
    "Val = BuildingDataset(png_dir = ValIMG_ls,\n",
    "                      lbl_dir = ValLBL_ls)\n",
    "\n",
    "Test = BuildingDataset(png_dir = TestIMG_ls,\n",
    "                       lbl_dir = TestLBL_ls)\n",
    "\n",
    "print(len(Train.png_dir) == len(Train.lbl_dir))\n",
    "print(type(Train.png_dir), type(Train.lbl_dir))\n",
    "\n",
    "print(\"Total images and labels pair in DataLoader: {0}\".format(len(Train.png_dir) + len(Val.png_dir) + len(Test.png_dir)))\n",
    "\n",
    "print(\"Concatenated TRAINING images and labels pair: {0} :\".format(len(Train.png_dir)))\n",
    "for x, y in zip(Train.png_dir, Train.lbl_dir):    \n",
    "    print(f\"Image: {x}\", f\"Label: {y}\")\n",
    "\n",
    "print(\"Concatenated VALIDATION images and labels pair: {0} :\".format(len(Val.png_dir)))\n",
    "for x, y in zip(Val.png_dir, Val.lbl_dir):\n",
    "    print(f\"Image: {x}\", f\"Label: {y}\")\n",
    "\n",
    "print(\"Concatenated TESTING images: {0} and labels pair: {0} :\".format(len(Test.png_dir)))\n",
    "for x, y in zip(Test.png_dir, Test.lbl_dir):\n",
    "    print(f\"Image: {x}\", f\"Label: {y}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c717152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trimmed down version\n",
    "def training_loop1(n_epochs, optimizer, model, xp_name,\n",
    "                   loss_fn, in_channels, out_channels, train_loader,\n",
    "                   val_loader, checkpoint_freq, val_freq):\n",
    "\n",
    "    model = model.train()\n",
    "\n",
    "    for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "        \n",
    "        log_dir = os.path.abspath(\"/home/chris/Dropbox/HOTOSM/log\")\n",
    "        checkpointdir = os.path.abspath(\"/home/chris/Dropbox/HOTOSM/checkpoints\")\n",
    "        writer = SummaryWriter(os.path.join(log_dir, xp_name))\n",
    "\n",
    "        loss_train = 0.0\n",
    "\n",
    "        for i, (imgs, labels) in tqdm(enumerate(train_loader), total = len(train_loader)):\n",
    "            imgs = imgs.to(device = device, dtype = torch.float32)\n",
    "            labels = labels.to(device = device, dtype = torch.float32)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(imgs)\n",
    "            loss = loss_fn(prediction.squeeze(), labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "            global_step = epoch * len(train_loader) + i\n",
    "\n",
    "            if global_step % 10 == 0:\n",
    "                writer.add_scalar(\"Train/Loss\", loss.item(), global_step = global_step)\n",
    "\n",
    "        # Validation\n",
    "\n",
    "        if epoch % val_freq == 0:\n",
    "            \n",
    "            model = model.eval()\n",
    "            val_loss = 0.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for i, (imgs, labels) in tqdm(enumerate(val_loader), total = len(val_loader)):\n",
    "                    imgs = imgs.to(device = device, dtype = torch.float32)\n",
    "                    labels = labels.to(device = device, dtype = torch.float32)\n",
    "\n",
    "                    val_outIMG = model(imgs)\n",
    "                    prediction = torch.argmax(val_outIMG)\n",
    "                    val_loss += loss_fn(prediction, labels)\n",
    "                    assert val_loss.requires_grad == False\n",
    "\n",
    "                    if i == 0:\n",
    "                        labels = labels.cpu().detach().numpy()\n",
    "                        writer.add_images(\"Val/Sample_LBL\", np.rollaxis(labels.astype(np.uint8), 3, 1),\n",
    "                                          global_step = global_step)\n",
    "\n",
    "                        if out_channels > 1:\n",
    "                            writer.add_images(\"Val/Sample_conf_1\", prediction[:, 0, :, :].unsqueeze(1), \n",
    "                                              global_step = global_step)\n",
    "                            writer.add_images(\"Val/Sample_conf_2\", prediction[:, 1, :, :].unsqueeze(1), \n",
    "                                              global_step = global_step)\n",
    "\n",
    "                            confidence = prediction[:, 0, :, :] - prediction[:, 1, :, :]\n",
    "                            writer.add_images(\"Val/Sample_conf\", confidence.unsqueeze(1), \n",
    "                                              global_step = global_step)\n",
    "\n",
    "                            prediction = torch.argmax(prediction, 1).cpu().detach().numpy()\n",
    "                        else:\n",
    "                            writer.add_images(\"Val/Sample_conf\", confidence.unsqueeze(1), \n",
    "                                              global_step = global_step)\n",
    "\n",
    "                            prediction = (torch.sigmoid(prediction).cpu().detach().numpy() > 0.5).astype(np.uint8)\n",
    "\n",
    "                        writer.add_images(\"Val/Sample_pred\", np.rollaxis(prediction, 3, 1), \n",
    "                                          global_step = global_step)\n",
    "\n",
    "                if global_step % 10 == 0:\n",
    "                    writer.add_scalar(\"Val/Loss\", val_loss.item(), global_step = global_step)\n",
    "\n",
    "\n",
    "            if epoch % checkpoint_freq == 0:\n",
    "                os.makedirs(os.path.join(checkpointdir, xp_name), exist_ok = True)\n",
    "                checkpoint_file = os.path.join(checkpointdir, xp_name, xp_name + \"_iter_\" + str(global_step).zfill(6) + \".pth\")\n",
    "                model_states = {}\n",
    "\n",
    "                model_state = model.state_dict()\n",
    "\n",
    "                state = {\"Model:\": model_state, \"Epoch:\": epoch, \"Steps:\": global_step}\n",
    "                torch.save(state, checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "141acde1-7c7d-438f-bc83-aec86c900a4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five_UNet(\n",
      "  (encoder1): Sequential(\n",
      "    (enc1conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc1relu1): ReLU(inplace=True)\n",
      "    (enc1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc1relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (encoder2): Sequential(\n",
      "    (enc2conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc2relu1): ReLU(inplace=True)\n",
      "    (enc2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc2relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (encoder3): Sequential(\n",
      "    (enc3conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc3relu1): ReLU(inplace=True)\n",
      "    (enc3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc3relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (encoder4): Sequential(\n",
      "    (enc4conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc4relu1): ReLU(inplace=True)\n",
      "    (enc4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc4relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (encoder5): Sequential(\n",
      "    (enc5conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc5norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc5relu1): ReLU(inplace=True)\n",
      "    (enc5conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc5norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc5relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bottleneck): Sequential(\n",
      "    (bottleneckconv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bottlenecknorm1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bottleneckrelu1): ReLU(inplace=True)\n",
      "    (bottleneckconv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bottlenecknorm2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bottleneckrelu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv5): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder5): Sequential(\n",
      "    (dec5conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec5norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec5relu1): ReLU(inplace=True)\n",
      "    (dec5conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec5norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec5relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv4): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder4): Sequential(\n",
      "    (dec4conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec4relu1): ReLU(inplace=True)\n",
      "    (dec4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec4relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder3): Sequential(\n",
      "    (dec3conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec3relu1): ReLU(inplace=True)\n",
      "    (dec3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec3relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder2): Sequential(\n",
      "    (dec2conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec2relu1): ReLU(inplace=True)\n",
      "    (dec2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec2relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv1): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder1): Sequential(\n",
      "    (dec1conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec1relu1): ReLU(inplace=True)\n",
      "    (dec1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec1relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "Trainable parameters in current model: [864, 32, 32, 9216, 32, 32, 18432, 64, 64, 36864, 64, 64, 73728, 128, 128, 147456, 128, 128, 294912, 256, 256, 589824, 256, 256, 1179648, 512, 512, 2359296, 512, 512, 4718592, 1024, 1024, 9437184, 1024, 1024, 2097152, 512, 4718592, 512, 512, 2359296, 512, 512, 524288, 256, 1179648, 256, 256, 589824, 256, 256, 131072, 128, 294912, 128, 128, 147456, 128, 128, 32768, 64, 73728, 64, 64, 36864, 64, 64, 8192, 32, 18432, 32, 32, 9216, 32, 32, 64, 2]\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(Train, batch_size = 3, shuffle = False) \n",
    "val_loader = DataLoader(Val, batch_size = 3, shuffle = False)\n",
    "\n",
    "Net = Five_UNet()\n",
    "\n",
    "n_params = [p.numel() for p in Net.parameters() if p.requires_grad == True]\n",
    "\n",
    "print(Net)\n",
    "print('Trainable parameters in current model:', n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "098875cb-a2b4-4309-9d6c-a2976f7a88fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:23<00:00, 13.83s/it]\n",
      "100%|██████████| 6/6 [01:26<00:00, 14.41s/it]/it]\n",
      "  2%|▏         | 2/100 [02:49<2:18:53, 85.04s/it]"
     ]
    }
   ],
   "source": [
    "training_loop1(n_epochs = 100,\n",
    "              optimizer = torch.optim.Adam(Net.parameters(), lr = 1e-3, weight_decay = 1e-3),\n",
    "              model = Net,\n",
    "              in_channels = 3,\n",
    "              out_channels = 2,\n",
    "              xp_name = \"FIRST17:9_Adam1e-3_wd1e-3_b1_ep100_KLDivLoss\",\n",
    "              loss_fn = nn.KLDivLoss(reduction = 'batchmean'),\n",
    "              train_loader = train_loader,\n",
    "              val_loader = val_loader,\n",
    "              checkpoint_freq = 10,\n",
    "              val_freq = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
