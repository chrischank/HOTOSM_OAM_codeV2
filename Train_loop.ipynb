{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "994b046f-e5e7-48c9-9afd-a510f864b5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "#Training loop for NNs       #\n",
    "#Maintainer: Christopher Chan#\n",
    "#Version: 0.1.0              #\n",
    "#Date: 2022-02-23            #\n",
    "##############################\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pathlib\n",
    "import time\n",
    "import re\n",
    "import PIL\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "from PIL import Image\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from Networks import Five_UNet, Four_UNet\n",
    "from dataloader import BuildingDataset\n",
    "\n",
    "device = (torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "          else torch.device(\"cpu\"))\n",
    "\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "#td_KBY = os.path.abspath(\"/home/chris/Dropbox/HOTOSM/SAMPLE/td_KBY\")\n",
    "#td_DZK = os.path.abspath(\"/home/chris/Dropbox/HOTOSM/SAMPLE/td_DZK\")\n",
    "#td_DZKN = os.path.abspath(\"/home/chris/Dropbox/HOTOSM/SAMPLE/td_DZKN\")\n",
    "\n",
    "td_KBY10 = os.path.abspath(\"/home/mnt/HOTOSM_data/Kakuma10cm/Kalobeyei/td_KBY\")\n",
    "td_DZK10 = os.path.abspath(\"/home/mnt/HOTOSM_data/Dzaleka10cm/td_DZK\")\n",
    "td_DZKN10 = os.path.abspath(\"/home/mnt/HOTOSM_data/Dzaleka_N10cm/td_DZKN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98591c8",
   "metadata": {},
   "source": [
    "## Change class label from 2 to 1 for binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afff1f48",
   "metadata": {},
   "source": [
    "for root, dirs, filename in os.walk(os.path.join(td_DZK10, \"LBL\")):\n",
    "    for i in filename:\n",
    "        if i.endswith(\".png\"):\n",
    "            png = Image.open(root + \"/\" + i)\n",
    "            px = png.load()\n",
    "\n",
    "            for x in range(png.size[0]):\n",
    "                for y in range(png.size[1]):\n",
    "                    if px[x, y] == 2:\n",
    "                        px[x, y] = 1\n",
    "                        \n",
    "                png.save(root + \"/\" + i, \"PNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba42bb2-e3e5-4b82-8757-2c591377fa0c",
   "metadata": {},
   "source": [
    "### Train Val Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2cfbd46-14a4-46d0-955c-8bcb85c3d3f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images and labels pair in DataLoader: 2890\n",
      "Concatenated TRAINING images and labels pair: 1734 :\n",
      "Concatenated VALIDATION images and labels pair: 867 :\n",
      "Concatenated TESTING images: 289 and labels pair: 289 :\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Below is a set of relatively complex functions which:\n",
    "# Perform the train, val, test split at a rounded ratio of 62%, 27%, and 10% based on each sets of imagery\n",
    "# This will be followed by first pseudo changing the name of _LBL_ to _IMG_ to match the split imagery\n",
    "# Lastly, once the correct LBL files are matched, \n",
    "\n",
    "def tvt_split(td):\n",
    "    \n",
    "    img_ls = []\n",
    "    \n",
    "    for root, dirs, filename in os.walk(os.path.join(td, \"IMG\")):\n",
    "        for i in filename:\n",
    "            if i.endswith(\".png\"):\n",
    "                img_ls.append(root + \"/\" + i)\n",
    "        \n",
    "        img_ls = BuildingDataset(img_ls, _)\n",
    "        \n",
    "        train_IMG, val_IMG, test_IMG = random_split(img_ls.png_dir, [int(round(0.6 * len(img_ls.png_dir))),\n",
    "                                                                     int(round(0.3 * len(img_ls.png_dir))),\n",
    "                                                                     int(round(0.1 * len(img_ls.png_dir)))])\n",
    "        \n",
    "        return train_IMG, val_IMG, test_IMG\n",
    "\n",
    "DZK_train, DZK_val, DZK_test = tvt_split(td_DZK10)\n",
    "KBY_train, KBY_val, KBY_test = tvt_split(td_KBY10)\n",
    "DZKN_train, DZKN_val, DZKN_test = tvt_split(td_DZKN10)\n",
    "\n",
    "##################\n",
    "# TOO COMPLICATED#\n",
    "##################\n",
    "\n",
    "#def match_LBL(td, imgs):\n",
    "#    \n",
    "#    lbl_ls = []\n",
    "#    img_ls = []\n",
    "#    match_ls = []\n",
    "#    \n",
    "#    imgs = list(imgs)\n",
    "#    \n",
    "#    for root, dirs, filename in os.walk(os.path.join(td, \"LBL\")):\n",
    "#        for j in filename:\n",
    "#            if j.endswith(\".png\"):\n",
    "#                ps_name = j.rsplit(\"_LBL_\")[0] + \"_IMG_\" + j.rsplit(\"_LBL_\")[1] # Parse the string, PSEUDO-CHANGE _LBL_ to _IMG_\n",
    "#                lbl_ls.append(ps_name)\n",
    "#    \n",
    "#    for k in imgs:\n",
    "#        names = os.path.basename(k)\n",
    "#        img_ls.append(names)\n",
    "#        \n",
    "#    def common(a, b):\n",
    "#        a_set = set(a)\n",
    "#        b_set = set(b)\n",
    "#        if (a_set & b_set):\n",
    "#            return (a_set & b_set)\n",
    "#        else:\n",
    "#            print(\"No common elements\")\n",
    "#            \n",
    "#            \n",
    "#    match_ls = common(img_ls, lbl_ls)\n",
    "#        \n",
    "#    match_ls = [(root + \"/\" + n.replace(\"_IMG_\", \"_LBL_\")) for n in match_ls] # Change the _IMG_ back to _LBL_\n",
    "#    \n",
    "#    print(\"For the selected dataset of {0}, There are: {1} images, {2} labels, and {3} matching image/label pairs.\".format(os.path.basename(td), len(img_ls), len(lbl_ls), len(match_ls)))\n",
    "#    \n",
    "#    return match_ls\n",
    "\n",
    "#########################################\n",
    "# Assign matched LBL to new LBL datasets#\n",
    "#########################################\n",
    "\n",
    "#DZKLBL_Train = match_LBL(td_DZK, DZK_train)\n",
    "#DZKLBL_Val = match_LBL(td_DZK, DZK_val)\n",
    "#DZKLBL_Test = match_LBL(td_DZK, DZK_test)\n",
    "#DZKNLBL_Train = match_LBL(td_DZKN, DZKN_train)\n",
    "#DZKNLBL_Val = match_LBL(td_DZKN, DZKN_val)\n",
    "#DZKNLBL_Test = match_LBL(td_DZKN, DZKN_test)\n",
    "#KBYLBL_Train = match_LBL(td_KBY, KBY_train)\n",
    "#KBYLBL_Val = match_LBL(td_KBY, KBY_val)\n",
    "#KBYLBL_Test = match_LBL(td_KBY, KBY_test)\n",
    "\n",
    "############\n",
    "# Try again#\n",
    "############\n",
    "\n",
    "TrainLBL_ls = []\n",
    "ValLBL_ls = []\n",
    "TestLBL_ls = []\n",
    "\n",
    "TrainIMG_ls = list(DZK_train + KBY_train + DZKN_train)\n",
    "ValIMG_ls = list(DZK_val + KBY_val + DZKN_val)\n",
    "TestIMG_ls = list(DZK_test + KBY_test + DZKN_test)\n",
    "\n",
    "for i in TrainIMG_ls:\n",
    "    i = re.sub(\"IMG\", \"LBL\", i, count = 2)\n",
    "    TrainLBL_ls.append(i)\n",
    "\n",
    "for i in ValIMG_ls:\n",
    "    i = re.sub(\"IMG\", \"LBL\", i, count = 2)\n",
    "    ValLBL_ls.append(i)\n",
    "\n",
    "for i in TestIMG_ls:\n",
    "    i = re.sub(\"IMG\", \"LBL\", i, count = 2)\n",
    "    TestLBL_ls.append(i)\n",
    "\n",
    "Train = BuildingDataset(png_dir = TrainIMG_ls,\n",
    "                        lbl_dir = TrainLBL_ls)\n",
    "\n",
    "Val = BuildingDataset(png_dir = ValIMG_ls,\n",
    "                      lbl_dir = ValLBL_ls)\n",
    "\n",
    "Test = BuildingDataset(png_dir = TestIMG_ls,\n",
    "                       lbl_dir = TestLBL_ls)\n",
    "\n",
    "assert len(Train.png_dir) == len(Train.lbl_dir)\n",
    "\n",
    "print(\"Total images and labels pair in DataLoader: {0}\".format(len(Train.png_dir) + len(Val.png_dir) + len(Test.png_dir)))\n",
    "\n",
    "print(\"Concatenated TRAINING images and labels pair: {0} :\".format(len(Train.png_dir)))\n",
    "#for x, y in zip(Train.png_dir, Train.lbl_dir):    \n",
    "#    print(f\"Image: {x}\", f\"Label: {y}\")\n",
    "\n",
    "print(\"Concatenated VALIDATION images and labels pair: {0} :\".format(len(Val.png_dir)))\n",
    "#for x, y in zip(Val.png_dir, Val.lbl_dir):\n",
    "#    print(f\"Image: {x}\", f\"Label: {y}\")\n",
    "\n",
    "print(\"Concatenated TESTING images: {0} and labels pair: {0} :\".format(len(Test.png_dir)))\n",
    "#for x, y in zip(Test.png_dir, Test.lbl_dir):\n",
    "#    print(f\"Image: {x}\", f\"Label: {y}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c717152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trimmed down version\n",
    "def training_loop1(n_epochs, optimizer, model, xp_name,\n",
    "                   loss_fn, in_channels, out_channels, train_loader,\n",
    "                   val_loader, checkpoint_freq, val_freq):\n",
    "\n",
    "    model = model.train()\n",
    "\n",
    "    for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "        \n",
    "        log_dir = os.path.abspath(\"/home/mnt/HOTOSM_data/log\")\n",
    "        checkpointdir = os.path.abspath(\"/home/mnt/HOTOSM_data/checkpoints\")\n",
    "        writer = SummaryWriter(os.path.join(log_dir, xp_name))\n",
    "\n",
    "        loss_train = 0.0\n",
    "\n",
    "        for i, (imgs, labels) in tqdm(enumerate(train_loader), total = len(train_loader)):\n",
    "            imgs = imgs.to(device = device, dtype = torch.float32)\n",
    "            labels = labels.to(device = device, dtype = torch.float32)\n",
    "\n",
    "            prediction = model(imgs)\n",
    "            loss = loss_fn(prediction.squeeze(0), labels.squeeze(0))\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss_train += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            global_step = epoch * len(train_loader) + i\n",
    "\n",
    "            if global_step % 10 == 0:\n",
    "                writer.add_scalar(\"Train/Loss\", loss.item(), global_step = global_step)\n",
    "\n",
    "        # Validation\n",
    "\n",
    "        if epoch % val_freq == 0:\n",
    "            \n",
    "            model = model.eval()\n",
    "            val_loss = 0.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for i, (imgs, labels) in tqdm(enumerate(val_loader), total = len(val_loader)):\n",
    "                    imgs = imgs.to(device = device, dtype = torch.float32)\n",
    "                    labels = labels.to(device = device, dtype = torch.float32)\n",
    "\n",
    "                    prediction = model(imgs)\n",
    "                    val_loss += loss_fn(prediction.squeeze(0), labels.squeeze(0))\n",
    "                    assert val_loss.requires_grad == False\n",
    "\n",
    "#                    if i == 0:\n",
    "#                        writer.add_images(\"Val/Sample_LBL\", labels[:, :, :, :].squeeze(0), dataformats = \"CHW\",\n",
    "#                                          global_step = global_step)\n",
    "#\n",
    "#                        if out_channels > 1:\n",
    "#                            writer.add_images(\"Val/Sample_conf_1\", prediction[:, 0, :, :].squeeze(0), \n",
    "#                                              global_step = global_step, dataformats = \"CHW\")\n",
    "#                            writer.add_images(\"Val/Sample_conf_2\", prediction[:, 1, :, :].squeeze(0), \n",
    "#                                              global_step = global_step, dataformats = \"CHW\")\n",
    "#\n",
    "#                            confidence = prediction[:, 0, :, :] - prediction[:, 1, :, :]\n",
    "#                            writer.add_images(\"Val/Sample_conf\", confidence.squeeze(0), \n",
    "#                                              global_step = global_step, dataformats = \"CHW\")\n",
    "#\n",
    "#                            prediction = torch.argmax(prediction, 1).cpu().detach().numpy()\n",
    "#                        else:\n",
    "#                            writer.add_images(\"Val/Sample_conf\", confidence.squeeze(0), \n",
    "#                                              global_step = global_step, dataformats = \"CHW\")\n",
    "#\n",
    "#                            prediction = (torch.sigmoid(prediction) > 0.5)\n",
    "#\n",
    "#                        writer.add_images(\"Val/Sample_pred\", prediction.squeeze(0), \n",
    "#                                          global_step = global_step, dataformats = \"CHW\")\n",
    "#\n",
    "                writer.add_scalar(\"Val/Loss\", val_loss.item(), global_step = global_step)\n",
    "\n",
    "\n",
    "        if epoch % checkpoint_freq == 0:\n",
    "            os.makedirs(os.path.join(checkpointdir, xp_name), exist_ok = True)\n",
    "            checkpoint_file = os.path.join(checkpointdir, xp_name, xp_name + \"_iter_\" + str(global_step).zfill(6) + \".pth\")\n",
    "            model_state = {}\n",
    "\n",
    "            model_state = model.state_dict()\n",
    "\n",
    "            state = {\"Model:\": model_state, \"Epoch:\": epoch, \"Steps:\": global_step}\n",
    "            torch.save(state, checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "141acde1-7c7d-438f-bc83-aec86c900a4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Four_UNet(\n",
      "  (encoder1): Sequential(\n",
      "    (enc1conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc1relu1): ReLU(inplace=True)\n",
      "    (enc1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc1relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (encoder2): Sequential(\n",
      "    (enc2conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc2relu1): ReLU(inplace=True)\n",
      "    (enc2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc2relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (encoder3): Sequential(\n",
      "    (enc3conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc3relu1): ReLU(inplace=True)\n",
      "    (enc3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc3relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (encoder4): Sequential(\n",
      "    (enc4conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc4relu1): ReLU(inplace=True)\n",
      "    (enc4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (enc4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (enc4relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bottleneck): Sequential(\n",
      "    (bottleneckconv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bottlenecknorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bottleneckrelu1): ReLU(inplace=True)\n",
      "    (bottleneckconv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bottlenecknorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bottleneckrelu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv4): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder4): Sequential(\n",
      "    (dec5conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec5norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec5relu1): ReLU(inplace=True)\n",
      "    (dec5conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec5norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec5relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder3): Sequential(\n",
      "    (dec4conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec4norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec4relu1): ReLU(inplace=True)\n",
      "    (dec4conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec4norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec4relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder2): Sequential(\n",
      "    (dec3conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec3norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec3relu1): ReLU(inplace=True)\n",
      "    (dec3conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec3norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec3relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (upconv1): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder1): Sequential(\n",
      "    (dec1conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec1relu1): ReLU(inplace=True)\n",
      "    (dec1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (dec1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dec1relu2): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "Trainable parameters in current model: [864, 32, 32, 9216, 32, 32, 18432, 64, 64, 36864, 64, 64, 73728, 128, 128, 147456, 128, 128, 294912, 256, 256, 589824, 256, 256, 1179648, 512, 512, 2359296, 512, 512, 524288, 256, 1179648, 256, 256, 589824, 256, 256, 131072, 128, 294912, 128, 128, 147456, 128, 128, 32768, 64, 73728, 64, 64, 36864, 64, 64, 8192, 32, 18432, 32, 32, 9216, 32, 32, 32, 1]\n"
     ]
    }
   ],
   "source": [
    "Net = Four_UNet()\n",
    "Net = Net.to(device = device)\n",
    "\n",
    "n_params = [p.numel() for p in Net.parameters() if p.requires_grad == True]\n",
    "\n",
    "print(Net)\n",
    "print('Trainable parameters in current model:', n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "098875cb-a2b4-4309-9d6c-a2976f7a88fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/217 [00:00<?, ?it/s]\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10492/1552464795.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m training_loop1(n_epochs = 500,\n\u001b[0m\u001b[1;32m      2\u001b[0m               \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m               \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m               \u001b[0min_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0mout_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_10492/1344329896.py\u001b[0m in \u001b[0;36mtraining_loop1\u001b[0;34m(n_epochs, optimizer, model, xp_name, loss_fn, in_channels, out_channels, train_loader, val_loader, checkpoint_freq, val_freq)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mloss_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "training_loop1(n_epochs = 500,\n",
    "              optimizer = torch.optim.Adam(Net.parameters(), lr = 1e-3, weight_decay = 1e-3),\n",
    "              model = Net,\n",
    "              in_channels = 3,\n",
    "              out_channels = 1,\n",
    "              xp_name = \"1734:867oc1_FourUNET_Adam1e-3_wd1e-3_b8_ep500_BCELogit\",\n",
    "              loss_fn = nn.BCEWithLogitsLoss(reduction = 'mean'),\n",
    "              train_loader = DataLoader(Train, batch_size = 8, shuffle = False),\n",
    "              val_loader = DataLoader(Val, batch_size = 8, shuffle = False),\n",
    "              checkpoint_freq = 10,\n",
    "              val_freq = 10)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561a2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
