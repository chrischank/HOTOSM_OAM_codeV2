Hardware,Nvidia GeForce 1070 Ti 8GB,,,,,,,,,,,,
Dimensions,512 * 512 * 0.1,,256 * 256 * 0.15,,,,,,,,,,
Train_data(BASE),2368,,2595,,,,,,,,,,
Train_data(AUG),9472,,11836,,,,,,,,,,
Val_data,1184,,1698,,,,,,,,,,
Test_data,,,611,,,,,,,,,,
Optimiser,Adam,,,,,,,,,,,,
Threshold,0.001,,,,,,,,,,,,
,,,,,,,,,,,,,
Date,Four_Unet(Vanilla),batch_size,pixel_size,window,Dataset,lr,wd,Scheduler,mean_OA,mean_Dice,mean_IoU,Binary_threshold,Comment
DAF2022-03-21,11836:1698_256oc1_Four-Unet_lr1e-3_wd1e-5_b32_ep500_BCE_RLRONPLATEAU(min1e-8)_iter_085469.pth,32,0.15,256,KBY + DZK + DZKN,0.001,0.00001,ReduceLRonPlateau(min1e-8)[p10],0.698,0.006,0.003,0.001,Best performing base run architecture so far
DAF2022-04-03,KBY3969:462_256oc1_Four-Unet_lr1e-3_wd1e-5_b32_ep500_BCE_RLRONPLATEAU(min1e-8)_iter_051155.pth,32,0.15,256,KBY,0.001,0.00001,ReduceLRonPlateau(min1e-8)[p10],0.903,0.007,0.003,0.001,Base KBY perfect dataset run
2022-04-15,20713:5094_256oc1_Four-Unet_lr1e-3_wd1e-5_b32_ep500_BCE_RLRONPLATEAU(min1e-8)_iter_272807.pth,32,0.15,256,KBY + DZK + DZKN,0.001,0.00001,ReduceLRonPlateau(min1e-8)[p20],0.698,0.006,0.003,0.001,"Changed reduce learning rate on plateau with patient at 20 epochs working out much better, initial 100 epochs val_loss decreased quickly"
,,,,,,,,,,,,,
,,,,,,,,,,,,,
,,,,,,,,,,,,,
,,,,,,,,,,,,,
,,,,,,,,,,,,,
,,,,,,,,,,,,,
,,,,,,,,,,,,,
Date,EB1-Unet(ImageNet),,,,,,,,,,,,
DAF2022-03-25,11836:1698_256oc1_EB1-UNet-IMN_lr1e-3_wd1e-5_b32_ep500_BCE_RLRONPLATEAU(min1e-8)_iter_115069.pth,32,0.15,256,KBY + DZK + DZKN,0.001,0.00001,ReduceLRonPlateau(min1e-8)[p10],0.698,0.006,0.003,0.001,"First run on 32 batch_size with 256 by 256 by 0.15. This has a much larger Train Val set, picking up bad roofs in Dzaleka quite well, but not confident"
DAF2022-04-05,KBY3969:462_256oc1_EB1-UNet-IMN_lr1e-3_wd1e-5_b32_ep500_BCE_RLRONPLATEAU(min1e-8)_iter_058115.pth,32,0.15,256,KBY,0.001,0.00001,ReduceLRonPlateau(min1e-8)[p10],0.903,0.7,0.003,0.001,"Still decreasuing, comparable to 4-layer U-Net"
2022-04-13,20713:5094_256oc1_EB1-Unet-IMNlr1e-3_wd1e-5_b32_ep500_BCE_RLRONPLATEAU(min1e-8)_iter_324647.pth,32,0.15,256,KBY + DZK + DZKN,0.001,0.00001,ReduceLRonPlateau(min1e-8)[p10],0.69,0.006,0.003,0.001,"First full augmentation run. The val_loss have yet to plateau after 500 epochs, could make reduce learning rate on plateau patience larger"
2022-04-18,20713:5094_256oc1_EB1-Unet-IMNlr1e-3_wd1e-5_b32_ep500_BCE_RLRONPLATEAU(min1e-8)P20_iter_318167.pth,32,0.15,256,KBY + DZK + DZKN,0.001,0.00001,ReduceLRonPlateau(min1e-8)[p20],0.687,0.006,0.003,0.001,No significance increase in initial val_loss comparted to patience of 10 epochs in ReduceLRonPlateau()
,,,,,,,,,,,,,
,,,,,,,,,,,,,
,,,,,,,,,,,,,
Date,EB1-Unet(qubvel),,,,,,,,,,,,
DAF2022-03-28,11836:1698_256oc1_EB1-Unet-qubvel_lr1e-4_wd1e-5_b16_ep500_BCE_RLRONPLATEAU(min1e-8)_iter_141339.pth,16,0.15,256,KBY + DZK + DZKN,0.0001,0.00001,ReduceLRonPlateau(min1e-8)[p10],0.702,0.006,0.003,0.001,"Not really convergin, quite surprising since this is the winning network in OCC on drone images early stopped at 289 epochs"
DAF2022-04-06,KBY3969:462_256oc1_EB1-Unet-qubvel1e-4_wd1e-5_b16_ep500_BCE_RLRONPLATEAU(min1e-8)_iter_108800.pth,16,0.15,256,KBY,0.0001,0.00001,ReduceLRonPlateau(min1e-8)[p10],0.902,0.006,0.003,0.001,"No significance learning val loss compared to Vanilla and ImageNet based weigths, but decreasing potential"
,,,,,,,,,,,,,
,,,,,,,,,,,,,
,,,,,,,,,,,,,
,,,,,,,,,,,,,
,,,,,,,,,,,,,
,,,,,,,,,,,,,
,,,,,,,,,,,,,
Date,EB1-Unet(NoIMN),,,,,,,,,,,,
DAF2022-04-02,11836:1698_256oc1_EB1-UNet-NoIMN_lr1e-3_wd1e-5_b32_ep500_BCE_RLRONPLATEAU(min1e-8)_iter_166869.pth,32,0.15,256,KBY + DZK + DZKN,0.001,0.00001,ReduceLRonPlateau(min1e-8)[p10],0.696,0.006,0.003,0.001,"Val_loss less stable, but comparable values top IMN based and vanilla based training"
DAF2022-04-04,KBY3969:462_256oc1_EB1-UNet-NoIMN_lr1e-3_wd1e-5_b32_ep500_BCE_RLRONPLATEAU(min1e-8)_iter_027955.pth,32,0.15,256,KBY,0.001,0.00001,ReduceLRonPlateau(min1e-8)[p10],0.903,0.007,0.003,0.001,"Similar, but stagnate at ~0.3346 while 4-Unet would still have potential to decrease"
,,,,,,,,,,,,,
,,,,,,,,,,,,,
ALL TRAINING BEFORE 2022-04-09 DATA AUGMENTATION ARE FUTILE,,,,,,,,,,,,,
